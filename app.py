import streamlit as st
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, SystemMessage
from dotenv import load_dotenv
import os

# ç’°å¢ƒå¤‰æ•°ã®èª­ã¿è¾¼ã¿
load_dotenv()

# OpenAI APIã‚­ãƒ¼ã®è¨­å®š
openai_api_key = os.getenv("OPENAI_API_KEY")

# å°‚é–€å®¶ã®ç¨®é¡ã¨ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®å®šç¾©
EXPERT_TYPES = {
    "åŒ»ç™‚å°‚é–€å®¶": "ã‚ãªãŸã¯çµŒé¨“è±Šå¯ŒãªåŒ»ç™‚å°‚é–€å®¶ã§ã™ã€‚åŒ»å­¦çš„ãªçŸ¥è­˜ã«åŸºã¥ã„ã¦ã€æ­£ç¢ºã§åˆ†ã‹ã‚Šã‚„ã™ã„å›ç­”ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚",
    "æ³•å¾‹å°‚é–€å®¶": "ã‚ãªãŸã¯æ³•å¾‹ã®å°‚é–€å®¶ã§ã™ã€‚æ³•çš„ãªè¦³ç‚¹ã‹ã‚‰æ­£ç¢ºã§è©³ç´°ãªæƒ…å ±ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚",
    "é‡‘èå°‚é–€å®¶": "ã‚ãªãŸã¯é‡‘èãƒ»æŠ•è³‡ã®å°‚é–€å®¶ã§ã™ã€‚é‡‘èå¸‚å ´ã‚„æŠ•è³‡æˆ¦ç•¥ã«ã¤ã„ã¦å°‚é–€çš„ãªã‚¢ãƒ‰ãƒã‚¤ã‚¹ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚",
    "ITã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢": "ã‚ãªãŸã¯çµŒé¨“è±Šå¯ŒãªITã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ã§ã™ã€‚æŠ€è¡“çš„ãªå•é¡Œã«å¯¾ã—ã¦å®Ÿè·µçš„ãªè§£æ±ºç­–ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚",
    "ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°å°‚é–€å®¶": "ã‚ãªãŸã¯ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°ã®å°‚é–€å®¶ã§ã™ã€‚åŠ¹æœçš„ãªãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°æˆ¦ç•¥ã‚„ãƒ“ã‚¸ãƒã‚¹æˆé•·ã®ãŸã‚ã®ã‚¢ãƒ‰ãƒã‚¤ã‚¹ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚"
}


def get_llm_response(user_input: str, expert_type: str) -> tuple:
    """
    LLMã‹ã‚‰ã®å›ç­”ã‚’å–å¾—ã™ã‚‹é–¢æ•°
    
    Args:
        user_input (str): ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆ
        expert_type (str): é¸æŠã•ã‚ŒãŸå°‚é–€å®¶ã®ç¨®é¡
    
    Returns:
        tuple: (å›ç­”ãƒ†ã‚­ã‚¹ãƒˆ, å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³æ•°, å‡ºåŠ›ãƒˆãƒ¼ã‚¯ãƒ³æ•°, åˆè¨ˆãƒˆãƒ¼ã‚¯ãƒ³æ•°)
    """
    try:
        # ChatOpenAIã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã®ä½œæˆ
        llm = ChatOpenAI(
            model="gpt-4o-mini",
            temperature=0.7,
            openai_api_key=openai_api_key
        )
        
        # ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¨ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ä½œæˆ
        messages = [
            SystemMessage(content=EXPERT_TYPES[expert_type]),
            HumanMessage(content=user_input)
        ]
        
        # LLMã«å•ã„åˆã‚ã›
        response = llm.invoke(messages)
        
        # ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’å–å¾—
        token_usage = response.response_metadata.get('token_usage', {})
        prompt_tokens = token_usage.get('prompt_tokens', 0)
        completion_tokens = token_usage.get('completion_tokens', 0)
        total_tokens = token_usage.get('total_tokens', 0)
        
        return response.content, prompt_tokens, completion_tokens, total_tokens
    
    except Exception as e:
        return f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}", 0, 0, 0


# Streamlitã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®ãƒ¡ã‚¤ãƒ³éƒ¨åˆ†
def main():
    # ãƒšãƒ¼ã‚¸è¨­å®š
    st.set_page_config(
        page_title="AIå°‚é–€å®¶ç›¸è«‡ã‚¢ãƒ—ãƒª",
        page_icon="ğŸ¤–",
        layout="wide"
    )
    
    # ã‚¿ã‚¤ãƒˆãƒ«
    st.title("ğŸ¤– AIå°‚é–€å®¶ç›¸è«‡ã‚¢ãƒ—ãƒª")
    
    # ã‚¢ãƒ—ãƒªã®æ¦‚è¦èª¬æ˜
    st.markdown("""
    ### ğŸ“– ã‚¢ãƒ—ãƒªã®æ¦‚è¦
    ã“ã®ã‚¢ãƒ—ãƒªã¯ã€æ§˜ã€…ãªåˆ†é‡ã®å°‚é–€å®¶ã¨ã—ã¦AIã«ç›¸è«‡ã§ãã‚‹ã‚µãƒ¼ãƒ“ã‚¹ã§ã™ã€‚
    è³ªå•ã—ãŸã„åˆ†é‡ã®å°‚é–€å®¶ã‚’é¸æŠã—ã¦ã€è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚
    
    ### ğŸ”§ ä½¿ã„æ–¹
    1. ãƒ©ã‚¸ã‚ªãƒœã‚¿ãƒ³ã‹ã‚‰ç›¸è«‡ã—ãŸã„å°‚é–€å®¶ã®ç¨®é¡ã‚’é¸æŠã—ã¦ãã ã•ã„
    2. ãƒ†ã‚­ã‚¹ãƒˆãƒœãƒƒã‚¯ã‚¹ã«è³ªå•ã‚„ç›¸è«‡å†…å®¹ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„
    3. ã€Œé€ä¿¡ã€ãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯ã™ã‚‹ã¨ã€AIãŒå°‚é–€å®¶ã¨ã—ã¦å›ç­”ã—ã¾ã™
    
    ---
    """)
    
    # ã‚µã‚¤ãƒ‰ãƒãƒ¼ã«å°‚é–€å®¶é¸æŠ
    st.sidebar.header("å°‚é–€å®¶ã‚’é¸æŠ")
    expert_type = st.sidebar.radio(
        "ç›¸è«‡ã—ãŸã„å°‚é–€å®¶ã®ç¨®é¡:",
        list(EXPERT_TYPES.keys())
    )
    
    # é¸æŠã•ã‚ŒãŸå°‚é–€å®¶ã®èª¬æ˜ã‚’è¡¨ç¤º
    st.sidebar.info(f"**é¸æŠä¸­:** {expert_type}")
    st.sidebar.write(EXPERT_TYPES[expert_type])
    
    # ãƒ¡ã‚¤ãƒ³ã‚¨ãƒªã‚¢ã«å…¥åŠ›ãƒ•ã‚©ãƒ¼ãƒ 
    st.subheader(f"ğŸ’¬ {expert_type}ã¸ã®ç›¸è«‡")
    
    # å…¥åŠ›ãƒ•ã‚©ãƒ¼ãƒ 
    user_input = st.text_area(
        "è³ªå•ã‚„ç›¸è«‡å†…å®¹ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„:",
        height=150,
        placeholder="ã“ã“ã«è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„..."
    )
    
    # é€ä¿¡ãƒœã‚¿ãƒ³
    if st.button("é€ä¿¡", type="primary"):
        if not user_input.strip():
            st.warning("âš ï¸ è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
        elif not openai_api_key:
            st.error("âŒ OpenAI APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚.envãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        else:
            # ãƒ­ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°è¡¨ç¤º
            with st.spinner("å›ç­”ã‚’ç”Ÿæˆä¸­..."):
                # LLMã‹ã‚‰ã®å›ç­”ã‚’å–å¾—
                response, prompt_tokens, completion_tokens, total_tokens = get_llm_response(user_input, expert_type)
            
            # å›ç­”ã‚’è¡¨ç¤º
            st.success("âœ… å›ç­”ãŒç”Ÿæˆã•ã‚Œã¾ã—ãŸ!")
            st.markdown("### ğŸ“ å›ç­”:")
            st.markdown(response)
            
            # ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’è¡¨ç¤º
            st.markdown("### ğŸ“Š ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡:")
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³", f"{prompt_tokens:,}")
            with col2:
                st.metric("å‡ºåŠ›ãƒˆãƒ¼ã‚¯ãƒ³", f"{completion_tokens:,}")
            with col3:
                st.metric("åˆè¨ˆãƒˆãƒ¼ã‚¯ãƒ³", f"{total_tokens:,}")
            
            # è¿½åŠ æƒ…å ±
            st.info(f"ğŸ’¡ ã“ã®å›ç­”ã¯{expert_type}ã®è¦–ç‚¹ã‹ã‚‰æä¾›ã•ã‚Œã¦ã„ã¾ã™ã€‚")


if __name__ == "__main__":
    main()
